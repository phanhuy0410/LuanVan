{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ImlSUE1Fj08MVJIPHbPasb9D0cGi6srr",
      "authorship_tag": "ABX9TyN6BaXq+AmsAw2wtNClGKNR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phanhuy0410/LuanVan/blob/main/Forensic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "from pathlib import Path\n",
        "\n",
        "BASE_DIR = Path(\"/content/drive/MyDrive/DARPA_TC\")\n",
        "OUTPUT_DIR = BASE_DIR / \"E3_extracted\"\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "tar_files = [\n",
        "    \"ta1-theia-e3-official-1r.json.tar.gz\",\n",
        "    \"ta1-theia-e3-official-3.json.tar.gz\",\n",
        "    \"ta1-theia-e3-official-5m.json.tar.gz\",\n",
        "    \"ta1-theia-e3-official-6r.json.tar.gz\"\n",
        "]\n",
        "\n",
        "for tar_name in tar_files:\n",
        "    scenario = tar_name.split(\"-\")[-1].replace(\".json.tar.gz\", \"\")\n",
        "    extract_dir = OUTPUT_DIR / scenario\n",
        "    extract_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    tar_path = BASE_DIR / tar_name\n",
        "    print(f\"Extracting {tar_name} ‚Üí {extract_dir}\")\n",
        "\n",
        "    with tarfile.open(tar_path, \"r:gz\") as tar:\n",
        "        tar.extractall(path=extract_dir)\n",
        "\n",
        "print(\"‚úÖ Done extracting all 4 files\")\n",
        "\n"
      ],
      "metadata": {
        "id": "1pAA877AUrro",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31aa3a46-1e41-4ee2-df81-a22a3c0e9e82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ta1-theia-e3-official-1r.json.tar.gz ‚Üí /content/drive/MyDrive/DARPA_TC/E3_extracted/1r\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1998673097.py:24: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(path=extract_dir)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ta1-theia-e3-official-3.json.tar.gz ‚Üí /content/drive/MyDrive/DARPA_TC/E3_extracted/3\n",
            "Extracting ta1-theia-e3-official-5m.json.tar.gz ‚Üí /content/drive/MyDrive/DARPA_TC/E3_extracted/5m\n",
            "Extracting ta1-theia-e3-official-6r.json.tar.gz ‚Üí /content/drive/MyDrive/DARPA_TC/E3_extracted/6r\n",
            "‚úÖ Done extracting all 4 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "DATA_DIR = Path(\"/content/drive/MyDrive/DARPA_TC/E3_extracted\")\n",
        "LABEL_FILE = Path(\"/content/drive/MyDrive/DARPA_TC/multiclass.json\")\n",
        "OUTPUT_PARQUET = \"/content/drive/MyDrive/DARPA_TC/darpaTC_theia_labeled.parquet\"\n",
        "\n",
        "BATCH_SIZE = 100_000\n",
        "with open(LABEL_FILE) as f:\n",
        "    label_spec = json.load(f)\n",
        "\n",
        "LABEL_MAPPING = {\n",
        "    int(k): v[\"name\"]\n",
        "    for k, v in label_spec[\"label_definition\"].items()\n",
        "}\n",
        "\n",
        "INDICATORS = label_spec[\"indicators\"]\n",
        "principal_info = {}\n",
        "\n",
        "def parse_principal(record):\n",
        "    datum = record.get(\"root\", {}).get(\"datum\", {})\n",
        "    p = datum.get(\"com.bbn.tc.schema.avro.cdm18.Principal\")\n",
        "    if not p:\n",
        "        return\n",
        "    principal_info[p[\"uuid\"]] = {\n",
        "        \"uid\": p.get(\"uid\"),\n",
        "        \"username\": p.get(\"username\")\n",
        "    }\n",
        "process_to_principal = {}\n",
        "\n",
        "def parse_subject(record):\n",
        "    datum = record.get(\"root\", {}).get(\"datum\", {})\n",
        "    s = datum.get(\"com.bbn.tc.schema.avro.cdm18.Subject\")\n",
        "    if not s:\n",
        "        return\n",
        "    puuid = s.get(\"principal\", {}).get(\"uuid\")\n",
        "    if puuid:\n",
        "        process_to_principal[s[\"uuid\"]] = puuid\n",
        "\n",
        "def match_label(proc_name, obj_path, obj_ip):\n",
        "    labels = []\n",
        "    for ind in INDICATORS:\n",
        "        if ind[\"type\"] == \"process_name\" and proc_name == ind[\"value\"]:\n",
        "            labels.append(ind[\"label\"])\n",
        "        elif ind[\"type\"] == \"file_path\" and obj_path == ind[\"value\"]:\n",
        "            labels.append(ind[\"label\"])\n",
        "        elif ind[\"type\"] == \"ip\" and obj_ip == ind[\"value\"]:\n",
        "            labels.append(ind[\"label\"])\n",
        "    return max(labels) if labels else 0\n",
        "\n",
        "def parse_event(record, scenario):\n",
        "    datum = record.get(\"root\", {}).get(\"datum\", {})\n",
        "    evt = datum.get(\"com.bbn.tc.schema.avro.cdm18.Event\")\n",
        "    if not evt:\n",
        "        return None\n",
        "\n",
        "    subj = evt.get(\"subject\", {})\n",
        "    obj = evt.get(\"object\", {})\n",
        "\n",
        "    proc_uuid = subj.get(\"uuid\")\n",
        "    proc_name = subj.get(\"name\")\n",
        "\n",
        "    obj_path = obj.get(\"path\")\n",
        "    obj_ip = obj.get(\"remoteAddress\") or obj.get(\"ip\")\n",
        "\n",
        "    label_id = match_label(proc_name, obj_path, obj_ip)\n",
        "\n",
        "    principal_uuid = process_to_principal.get(proc_uuid)\n",
        "    user = principal_info.get(principal_uuid, {})\n",
        "\n",
        "    return {\n",
        "        \"timestamp\": evt.get(\"timestampNanos\", 0) // 1_000_000_000,\n",
        "        \"operation\": evt.get(\"predicate\"),\n",
        "\n",
        "        \"proc_uuid\": proc_uuid,\n",
        "        \"proc_name\": proc_name,\n",
        "\n",
        "        \"user_uid\": user.get(\"uid\"),\n",
        "        \"username\": user.get(\"username\"),\n",
        "\n",
        "        \"obj_type\": obj.get(\"type\"),\n",
        "        \"obj_path\": obj_path,\n",
        "        \"obj_ip\": obj_ip,\n",
        "\n",
        "        \"label_id\": label_id,\n",
        "        \"label_name\": LABEL_MAPPING[label_id],\n",
        "        \"scenario\": scenario\n",
        "    }\n",
        "writer = None\n",
        "buffer = []\n",
        "\n",
        "# ---------- PASS 1 & 2 ----------\n",
        "print(\"üîç Building principal & process maps...\")\n",
        "for scenario_dir in DATA_DIR.iterdir():\n",
        "    if not scenario_dir.is_dir():\n",
        "        continue\n",
        "    for f in scenario_dir.glob(\"*.json*\"):\n",
        "        with open(f) as fh:\n",
        "            for line in fh:\n",
        "                rec = json.loads(line)\n",
        "                parse_principal(rec)\n",
        "                parse_subject(rec)\n",
        "\n",
        "# ---------- PASS 3 ----------\n",
        "print(\"üß† Parsing events & labeling...\")\n",
        "for scenario_dir in DATA_DIR.iterdir():\n",
        "    if not scenario_dir.is_dir():\n",
        "        continue\n",
        "\n",
        "    scenario = scenario_dir.name\n",
        "    for f in scenario_dir.glob(\"*.json*\"):\n",
        "        with open(f, buffering=1024*1024) as fh:\n",
        "            for line in tqdm(fh, desc=f.name):\n",
        "                rec = json.loads(line)\n",
        "                row = parse_event(rec, scenario)\n",
        "                if not row:\n",
        "                    continue\n",
        "\n",
        "                buffer.append(row)\n",
        "                if len(buffer) >= BATCH_SIZE:\n",
        "                    table = pa.Table.from_pylist(buffer)\n",
        "                    if writer is None:\n",
        "                        writer = pq.ParquetWriter(\n",
        "                            OUTPUT_PARQUET,\n",
        "                            table.schema,\n",
        "                            compression=\"snappy\"\n",
        "                        )\n",
        "                    writer.write_table(table)\n",
        "                    buffer.clear()\n",
        "\n",
        "# ---------- FLUSH ----------\n",
        "if buffer:\n",
        "    writer.write_table(pa.Table.from_pylist(buffer))\n",
        "if writer:\n",
        "    writer.close()\n",
        "\n",
        "print(\"‚úÖ DONE:\", OUTPUT_PARQUET)\n"
      ],
      "metadata": {
        "id": "A9DQZq1vrtrm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "395db6ec-285b-4f70-c018-b5b56aaae893"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Building principal & process maps...\n",
            "üß† Parsing events & labeling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ta1-theia-e3-official-1r.json: 5000000it [01:48, 45872.67it/s]\n",
            "ta1-theia-e3-official-1r.json.1: 5000000it [01:39, 50477.01it/s]\n",
            "ta1-theia-e3-official-1r.json.2: 5000000it [01:36, 51988.39it/s]\n",
            "ta1-theia-e3-official-1r.json.3: 5000000it [01:40, 49806.02it/s]\n",
            "ta1-theia-e3-official-1r.json.4: 5000000it [02:10, 38292.20it/s]\n",
            "ta1-theia-e3-official-1r.json.5: 5000000it [01:51, 44806.13it/s]\n",
            "ta1-theia-e3-official-1r.json.6: 5000000it [01:42, 48700.33it/s]\n",
            "ta1-theia-e3-official-1r.json.7: 5000000it [01:43, 48307.35it/s]\n",
            "ta1-theia-e3-official-1r.json.8: 5000000it [01:42, 48827.34it/s]\n",
            "ta1-theia-e3-official-1r.json.9: 1761949it [00:35, 49112.66it/s]\n",
            "ta1-theia-e3-official-3.json: 1588762it [00:32, 48681.76it/s]\n",
            "ta1-theia-e3-official-5m.json: 1090138it [00:23, 46310.84it/s]\n",
            "ta1-theia-e3-official-6r.json: 5000000it [01:55, 43405.67it/s]\n",
            "ta1-theia-e3-official-6r.json.1: 5000000it [01:42, 48970.54it/s]\n",
            "ta1-theia-e3-official-6r.json.2: 5000000it [01:37, 51511.64it/s]\n",
            "ta1-theia-e3-official-6r.json.3: 5000000it [01:34, 52897.33it/s]\n",
            "ta1-theia-e3-official-6r.json.4: 5000000it [01:39, 50316.33it/s]\n",
            "ta1-theia-e3-official-6r.json.5: 5000000it [01:40, 49997.58it/s]\n",
            "ta1-theia-e3-official-6r.json.6: 5000000it [02:01, 41157.38it/s]\n",
            "ta1-theia-e3-official-6r.json.7: 5000000it [01:54, 43480.26it/s]\n",
            "ta1-theia-e3-official-6r.json.8: 5000000it [01:38, 50878.45it/s]\n",
            "ta1-theia-e3-official-6r.json.9: 5000000it [01:46, 47084.79it/s]\n",
            "ta1-theia-e3-official-6r.json.10: 5000000it [01:39, 50070.42it/s]\n",
            "ta1-theia-e3-official-6r.json.11: 5000000it [01:38, 50955.84it/s]\n",
            "ta1-theia-e3-official-6r.json.12: 3852494it [01:54, 33742.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ DONE: /content/drive/MyDrive/DARPA_TC/darpaTC_theia_labeled.parquet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyarrow.parquet as pq\n",
        "from collections import Counter\n",
        "\n",
        "PARQUET_FILE = \"/content/drive/MyDrive/DARPA_TC/darpaTC_theia_labeled.parquet\"\n",
        "BATCH_SIZE = 100_000\n",
        "\n",
        "pf = pq.ParquetFile(PARQUET_FILE)\n",
        "\n",
        "label_counter = Counter()\n",
        "total_rows = 0\n",
        "\n",
        "for batch in pf.iter_batches(batch_size=BATCH_SIZE, columns=[\"label_name\"]):\n",
        "    df = batch.to_pandas()\n",
        "    label_counter.update(df[\"label_name\"])\n",
        "    total_rows += len(df)\n",
        "\n",
        "print(\"üìä T·ªïng s·ªë event:\", total_rows)\n",
        "print(\"üìä Ph√¢n b·ªë h√†nh vi:\")\n",
        "for label, cnt in label_counter.items():\n",
        "    print(f\"{label:30s} {cnt}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "cLb2P2pjrT_h",
        "outputId": "38a4c50e-cd91-4361-9cc5-8d1180ce6b5d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] Failed to open local file '/content/drive/MyDrive/DARPA_TC/darpaTC_theia_labeled.parquet'. Detail: [errno 2] No such file or directory",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3220779730.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100_000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mpf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParquetFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPARQUET_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mlabel_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyarrow/parquet/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, source, metadata, common_metadata, read_dictionary, memory_map, buffer_size, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, filesystem, page_checksum_verification)\u001b[0m\n\u001b[1;32m    311\u001b[0m             source, filesystem, memory_map=memory_map)\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilesystem\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m             \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilesystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_input_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_source\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# We opened it here, ensure we close it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyarrow/_fs.pyx\u001b[0m in \u001b[0;36mpyarrow._fs.FileSystem.open_input_file\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Failed to open local file '/content/drive/MyDrive/DARPA_TC/darpaTC_theia_labeled.parquet'. Detail: [errno 2] No such file or directory"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import gc\n",
        "\n",
        "# --- C·∫§U H√åNH ---\n",
        "DATA_DIR = Path(\"/content/drive/MyDrive/DARPA_TC/E3_extracted\")\n",
        "# File JSON ch·ª©a danh s√°ch nh√£n (Ground Truth)\n",
        "LABEL_FILE = Path(\"/content/drive/MyDrive/DARPA_TC/multiclass.json\")\n",
        "OUTPUT_PARQUET = \"/content/drive/MyDrive/DARPA_TC/darpaTC_theia_labeled.parquet\"\n",
        "BATCH_SIZE = 100_000\n",
        "\n",
        "# Load nh√£n\n",
        "with open(LABEL_FILE) as f:\n",
        "    label_spec = json.load(f)\n",
        "\n",
        "# Mapping ID -> Name (VD: 1 -> \"Initial Access\")\n",
        "LABEL_MAPPING = label_spec[\"label_mapping\"]\n",
        "INDICATORS = label_spec[\"indicators\"]\n",
        "\n",
        "# --- GLOBAL MAPS (L∆∞u tr√™n RAM ƒë·ªÉ tra c·ª©u nhanh) ---\n",
        "principal_info = {}       # User UUID -> {uid, username}\n",
        "subject_info = {}         # Subject UUID -> {principal_uuid, proc_name}\n",
        "object_info = {}          # Object UUID -> {path, ip, type}\n",
        "\n",
        "# H√†m tr·ª£ gi√∫p: L·∫•y n·ªôi dung b√™n trong \"datum\" an to√†n\n",
        "def get_datum(record):\n",
        "    # D·ª±a tr√™n h√¨nh ·∫£nh b·∫°n g·ª≠i: root -> datum\n",
        "    return record.get(\"root\", {}).get(\"datum\", {})\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# H√ÄM PASS 1: X√ÇY D·ª∞NG T·ª™ ƒêI·ªÇN (MAPPING UUID -> INFO)\n",
        "# ---------------------------------------------------------\n",
        "def parse_metadata(record):\n",
        "    datum = get_datum(record)\n",
        "    if not datum: return\n",
        "\n",
        "    # 1. Parse Principal (User)\n",
        "    p = datum.get(\"com.bbn.tc.schema.avro.cdm18.Principal\")\n",
        "    if p:\n",
        "        principal_info[p[\"uuid\"]] = {\n",
        "            \"uid\": p.get(\"userId\"),\n",
        "            \"username\": p.get(\"username\")\n",
        "        }\n",
        "        return\n",
        "\n",
        "    # 2. Parse Subject (Process) - Quan tr·ªçng ƒë·ªÉ bi·∫øt t√™n Process\n",
        "    s = datum.get(\"com.bbn.tc.schema.avro.cdm18.Subject\")\n",
        "    if s:\n",
        "        # L·∫•y t√™n process. ∆Øu ti√™n 'properties.map.path', fallback sang 'cmdLine'\n",
        "        proc_name = \"unknown\"\n",
        "\n",
        "        # Th·ª≠ l·∫•y path t·ª´ properties\n",
        "        props = s.get(\"properties\", {}).get(\"map\", {})\n",
        "        if props and \"path\" in props:\n",
        "             proc_name = str(props[\"path\"]).split('/')[-1]\n",
        "\n",
        "        # N·∫øu kh√¥ng c√≥ path, th·ª≠ cmdLine\n",
        "        elif s.get(\"cmdLine\"):\n",
        "             # CmdLine v√≠ d·ª•: \"/bin/bash -c ...\" -> l·∫•y \"bash\"\n",
        "             proc_name = str(s[\"cmdLine\"]).split(' ')[0].split('/')[-1]\n",
        "\n",
        "        subject_info[s[\"uuid\"]] = {\n",
        "            \"principal_uuid\": s.get(\"principal\", {}).get(\"uuid\") if isinstance(s.get(\"principal\"), dict) else s.get(\"principal\"),\n",
        "            \"proc_name\": proc_name\n",
        "        }\n",
        "        return\n",
        "\n",
        "    # 3. Parse FileObject - Quan tr·ªçng ƒë·ªÉ bi·∫øt file n√†o b·ªã truy c·∫≠p\n",
        "    f_obj = datum.get(\"com.bbn.tc.schema.avro.cdm18.FileObject\")\n",
        "    if f_obj:\n",
        "        # L·∫•y path t·ª´ properties\n",
        "        props = f_obj.get(\"properties\", {}).get(\"map\", {})\n",
        "        path = props.get(\"path\")\n",
        "        if path:\n",
        "            object_info[f_obj[\"uuid\"]] = {\"path\": path, \"type\": \"FILE\"}\n",
        "        return\n",
        "\n",
        "    # 4. Parse NetFlowObject - Quan tr·ªçng ƒë·ªÉ bi·∫øt IP k·∫øt n·ªëi\n",
        "    n_obj = datum.get(\"com.bbn.tc.schema.avro.cdm18.NetFlowObject\")\n",
        "    if n_obj:\n",
        "        ip = n_obj.get(\"remoteAddress\")\n",
        "        if ip:\n",
        "            object_info[n_obj[\"uuid\"]] = {\"ip\": ip, \"type\": \"NETFLOW\"}\n",
        "        return\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# H√ÄM PASS 2: X·ª¨ L√ù S·ª∞ KI·ªÜN (EVENT) & G√ÅN NH√ÉN\n",
        "# ---------------------------------------------------------\n",
        "def match_label(proc_name, obj_path, obj_ip):\n",
        "    labels = []\n",
        "    # D√πng key \"default_label\" kh·ªõp v·ªõi file JSON ground truth\n",
        "    target_key = \"default_label\"\n",
        "\n",
        "    for ind in INDICATORS:\n",
        "        # So s√°nh t√™n Process\n",
        "        if ind[\"type\"] == \"process_name\" and proc_name == ind[\"value\"]:\n",
        "            labels.append(ind[target_key])\n",
        "        # So s√°nh ƒë∆∞·ªùng d·∫´n File\n",
        "        elif ind[\"type\"] == \"file_path\" and obj_path == ind[\"value\"]:\n",
        "            labels.append(ind[target_key])\n",
        "        # So s√°nh IP\n",
        "        elif ind[\"type\"] == \"ip\" and obj_ip == ind[\"value\"]:\n",
        "            labels.append(ind[target_key])\n",
        "\n",
        "    # L·∫•y nh√£n cao nh·∫•t (m·ª©c ƒë·ªô nghi√™m tr·ªçng nh·∫•t)\n",
        "    return max(labels) if labels else 0\n",
        "\n",
        "def parse_event(record, scenario):\n",
        "    datum = get_datum(record)\n",
        "    evt = datum.get(\"com.bbn.tc.schema.avro.cdm18.Event\")\n",
        "\n",
        "    # N·∫øu d√≤ng n√†y kh√¥ng ph·∫£i Event th√¨ b·ªè qua\n",
        "    if not evt:\n",
        "        return None\n",
        "\n",
        "    # L·∫•y UUID Subject v√† Object t·ª´ Event\n",
        "    # CDM ƒë√¥i khi l∆∞u uuid d·∫°ng string, ƒë√¥i khi d·∫°ng dict {\"uuid\": \"...\"}\n",
        "    subj_raw = evt.get(\"subject\")\n",
        "    obj_raw = evt.get(\"object\")\n",
        "\n",
        "    subj_uuid = subj_raw.get(\"uuid\") if isinstance(subj_raw, dict) else subj_raw\n",
        "    obj_uuid = obj_raw.get(\"uuid\") if isinstance(obj_raw, dict) else obj_raw\n",
        "\n",
        "    # --- TRA C·ª®U TH√îNG TIN T·ª™ PASS 1 (LOOKUP) ---\n",
        "\n",
        "    # 1. Th√¥ng tin Process (Subject)\n",
        "    s_info = subject_info.get(subj_uuid, {})\n",
        "    proc_name = s_info.get(\"proc_name\", \"unknown\")\n",
        "    principal_uuid = s_info.get(\"principal_uuid\")\n",
        "\n",
        "    # 2. Th√¥ng tin User (Principal)\n",
        "    u_info = principal_info.get(principal_uuid, {})\n",
        "\n",
        "    # 3. Th√¥ng tin Object (File/IP)\n",
        "    o_info = object_info.get(obj_uuid, {})\n",
        "    obj_path = o_info.get(\"path\")\n",
        "    obj_ip = o_info.get(\"ip\")\n",
        "    obj_type = o_info.get(\"type\", \"UNKNOWN\")\n",
        "\n",
        "    # --- G√ÅN NH√ÉN ---\n",
        "    label_id = match_label(proc_name, obj_path, obj_ip)\n",
        "\n",
        "    return {\n",
        "        \"timestamp\": evt.get(\"timestampNanos\", 0) // 1_000_000_000,\n",
        "        \"operation\": evt.get(\"type\"), # V√≠ d·ª•: EVENT_READ, EVENT_WRITE\n",
        "\n",
        "        \"proc_uuid\": str(subj_uuid),\n",
        "        \"proc_name\": proc_name,\n",
        "\n",
        "        \"user_uid\": u_info.get(\"uid\"),\n",
        "        \"username\": u_info.get(\"username\"),\n",
        "\n",
        "        \"obj_type\": obj_type,\n",
        "        \"obj_path\": obj_path,\n",
        "        \"obj_ip\": obj_ip,\n",
        "\n",
        "        \"label_id\": label_id,\n",
        "        \"label_name\": LABEL_MAPPING.get(str(label_id), \"Benign\"),\n",
        "        \"scenario\": scenario\n",
        "    }\n",
        "\n",
        "# =========================================================\n",
        "# CH·∫†Y CH∆Ø∆†NG TR√åNH\n",
        "# =========================================================\n",
        "\n",
        "# T√¨m t·∫•t c·∫£ file JSON (lo·∫°i b·ªè file n√©n)\n",
        "files = sorted([p for p in DATA_DIR.glob(\"**/*.json*\") if \"tar.gz\" not in p.name])\n",
        "\n",
        "# --- PASS 1: Build Maps (ƒê·ªçc l∆∞·ªõt ƒë·ªÉ l·∫•y Metadata) ---\n",
        "print(f\"üîç PASS 1: Qu√©t Metadata t·ª´ {len(files)} files...\")\n",
        "for f in tqdm(files, desc=\"Metadata Scan\"):\n",
        "    with open(f, 'r', encoding='utf-8') as fh:\n",
        "        for line in fh:\n",
        "            try:\n",
        "                rec = json.loads(line)\n",
        "                parse_metadata(rec)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "print(f\"üìä ƒê√£ h·ªçc ƒë∆∞·ª£c: {len(principal_info)} Users, {len(subject_info)} Process, {len(object_info)} Objects\")\n",
        "\n",
        "# --- PASS 2: Parse Events & Write Parquet ---\n",
        "print(\"\\nüß† PASS 2: X·ª≠ l√Ω Events & G√°n nh√£n...\")\n",
        "writer = None\n",
        "buffer = []\n",
        "\n",
        "for f in files:\n",
        "    scenario = f.parent.name # L·∫•y t√™n th∆∞ m·ª•c l√†m t√™n k·ªãch b·∫£n\n",
        "\n",
        "    with open(f, 'r', encoding='utf-8') as fh:\n",
        "        for line in tqdm(fh, desc=f.name, leave=False):\n",
        "            try:\n",
        "                rec = json.loads(line)\n",
        "                row = parse_event(rec, scenario)\n",
        "\n",
        "                if row:\n",
        "                    buffer.append(row)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            # Ghi xu·ªëng ƒëƒ©a khi buffer ƒë·∫ßy (tr√°nh tr√†n RAM)\n",
        "            if len(buffer) >= BATCH_SIZE:\n",
        "                table = pa.Table.from_pylist(buffer)\n",
        "                if writer is None:\n",
        "                    writer = pq.ParquetWriter(OUTPUT_PARQUET, table.schema, compression=\"snappy\")\n",
        "                writer.write_table(table)\n",
        "                buffer.clear()\n",
        "\n",
        "        gc.collect() # D·ªçn r√°c b·ªô nh·ªõ sau m·ªói file\n",
        "\n",
        "# Ghi n·ªët ph·∫ßn c√≤n d∆∞\n",
        "if buffer:\n",
        "    table = pa.Table.from_pylist(buffer)\n",
        "    if writer is None:\n",
        "        writer = pq.ParquetWriter(OUTPUT_PARQUET, table.schema, compression=\"snappy\")\n",
        "    writer.write_table(table)\n",
        "\n",
        "if writer:\n",
        "    writer.close()\n",
        "\n",
        "print(f\"\\n‚úÖ HO√ÄN T·∫§T! File k·∫øt qu·∫£: {OUTPUT_PARQUET}\")"
      ],
      "metadata": {
        "id": "Jf7HCiZPpFcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S19X8Dytsm6F",
        "outputId": "e86802b0-d200-482f-c522-4fee354d72ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeGXzg7Hs7Hl",
        "outputId": "55860bee-ffb7-4c7f-e5ae-262647902e1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.12/dist-packages (2.7.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.13.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch-geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2025.11.12)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\n"
          ]
        }
      ]
    }
  ]
}